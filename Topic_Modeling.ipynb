{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KeyBERT or BERTopic?\n",
    "Both libraries use BERT to extract keywords from the documents in the dataset. However, they provide different data. MaartenGr, the owner of both libraries on Github, explains the differences between them in this link: https://coder.social/MaartenGr/KeyBERT/issues/60 . Unrelated, but interesting to note, is that he created both libraries and is based in Tilburg. This block provides a summary of the link.\n",
    "\n",
    "**About BERTopic:**\n",
    "*Steps*\n",
    "- Embedding documents\n",
    "- Clustering documents\n",
    "- Creating a topic representation.\n",
    "> The main output of BERTopic is a set of words per topic. Thus, multiple documents have the same topic representation.\n",
    "\n",
    "**About KeyBERT:**\n",
    "- Embedding documents\n",
    "- Creating candidate keywords\n",
    "- Calculating best keywords through either MMR, Max Sum Similarity, or Cosine Similarity\n",
    "> The main output of KeyBERT is a set of words per document. Thus, each document is expected to have different keywords.\n",
    "\n",
    "**Difference in output:**\n",
    "> BERTopic aims to cluster documents and create a broad representation of multiple documents whereas KeyBERT does not.\n",
    "\n",
    "**Finally, a note on when to use each:**\n",
    "> BERTopic, and in that sense most topic modeling techniques, are meant to explore the data to create an understanding of the perhaps millions of documents that you have collected. KeyBERT, in contrast, is not able to do this as it creates a completely different set of words per document. An example of using KeyBERT, and in that sense most keyword extraction algorithms, is automatically creating relevant keywords for content (blogs, articles, etc.) that businesses post on their website.\n",
    "\n",
    "In conclusion, we use KeyBERT for extracting keywords, and BERTopic to cluster the documents into topics. This should suffice for part 2's inputs: \"topics and keywords.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Download dataset of news articles from Reuters (English – from nltk.corpus import reuters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /home/musashishi/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Text processing\n",
    "import nltk\n",
    "nltk.download('reuters')\n",
    "from nltk.corpus import reuters as source\n",
    "\n",
    "# Keyword/topic extraction\n",
    "from keybert import KeyBERT\n",
    "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
    "\n",
    "# For editing the shape of structures\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>tag</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14826</td>\n",
       "      <td>test</td>\n",
       "      <td>ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14828</td>\n",
       "      <td>test</td>\n",
       "      <td>CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14829</td>\n",
       "      <td>test</td>\n",
       "      <td>JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14832</td>\n",
       "      <td>test</td>\n",
       "      <td>THAI TRADE DEFICIT WIDENS IN FIRST QUARTER\\n  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14833</td>\n",
       "      <td>test</td>\n",
       "      <td>INDONESIA SEES CPO PRICE RISING SHARPLY\\n  Ind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10783</th>\n",
       "      <td>999</td>\n",
       "      <td>training</td>\n",
       "      <td>U.K. MONEY MARKET SHORTAGE FORECAST REVISED DO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10784</th>\n",
       "      <td>9992</td>\n",
       "      <td>training</td>\n",
       "      <td>KNIGHT-RIDDER INC &amp;lt;KRN&gt; SETS QUARTERLY\\n  Q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10785</th>\n",
       "      <td>9993</td>\n",
       "      <td>training</td>\n",
       "      <td>TECHNITROL INC &amp;lt;TNL&gt; SETS QUARTERLY\\n  Qtly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10786</th>\n",
       "      <td>9994</td>\n",
       "      <td>training</td>\n",
       "      <td>NATIONWIDE CELLULAR SERVICE INC &amp;lt;NCEL&gt; 4TH ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10787</th>\n",
       "      <td>9995</td>\n",
       "      <td>training</td>\n",
       "      <td>&amp;lt;A.H.A. AUTOMOTIVE TECHNOLOGIES CORP&gt; YEAR ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10788 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      filename       tag                                               text\n",
       "0        14826      test  ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...\n",
       "1        14828      test  CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...\n",
       "2        14829      test  JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...\n",
       "3        14832      test  THAI TRADE DEFICIT WIDENS IN FIRST QUARTER\\n  ...\n",
       "4        14833      test  INDONESIA SEES CPO PRICE RISING SHARPLY\\n  Ind...\n",
       "...        ...       ...                                                ...\n",
       "10783      999  training  U.K. MONEY MARKET SHORTAGE FORECAST REVISED DO...\n",
       "10784     9992  training  KNIGHT-RIDDER INC &lt;KRN> SETS QUARTERLY\\n  Q...\n",
       "10785     9993  training  TECHNITROL INC &lt;TNL> SETS QUARTERLY\\n  Qtly...\n",
       "10786     9994  training  NATIONWIDE CELLULAR SERVICE INC &lt;NCEL> 4TH ...\n",
       "10787     9995  training  &lt;A.H.A. AUTOMOTIVE TECHNOLOGIES CORP> YEAR ...\n",
       "\n",
       "[10788 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert nltk corpus reuters to pandas\n",
    "reuters = []\n",
    "for fileid in source.fileids():\n",
    "    tag, filename = fileid.split('/')\n",
    "    reuters.append((filename, tag, source.raw(fileid)))\n",
    "\n",
    "df = pd.DataFrame(reuters, columns=['filename', 'tag', 'text'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert text to something useable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RIFT\\n  Mounting trade friction between the\\n  U.S. And Japan has raised fears among many of Asia's exportin\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO check stuff from the other notebooks\n",
    "df.iloc[0]['text'][:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(text: str):\n",
    "    # Remove escape characters\n",
    "    escapes = ''.join([chr(char) for char in range(1, 32)])\n",
    "    translator = str.maketrans('', '', escapes)\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    # Remove other weird characters \"\\'s\"\n",
    "    text = text.replace(\"\\'s\", \" \")\n",
    "    text = text.replace(\"n\\'t\", \" not\")\n",
    "    text = text.replace(\"-\", \" \")\n",
    "    text = text.replace(\"dlrs\", \" \")\n",
    "    text = text.replace(\"&lt;\", \" \")\n",
    "    text = text.replace(\"mln\", \"million\")\n",
    "    text = text.replace(\"pct\", \"percent\")\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) # remove all punctuation\n",
    "    text = text.lower()\n",
    "    text = ' '.join(text.split()) # remove long spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asian exporters fear damage from us japan rift mounting trade friction between the us and japan has raised fears among many of asia exporting nations '"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'] = df.apply(lambda row: cleanText(row['text']), axis=1)\n",
    "df.iloc[0]['clean_text'][:150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using KeyBERT to extract keywords from each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_model = KeyBERT()\n",
    "doc_embeddings, word_embeddings = kw_model.extract_embeddings(docs, min_df=1, stop_words=\"english\")\n",
    "# keywords = kw_model.extract_keywords(docs, min_df=1, stop_words=\"english\", \n",
    "#                                      doc_embeddings=doc_embeddings, \n",
    "#                                      word_embeddings=word_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BERTopic to extract topics from the corpus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37b43152a2d59eb046527c16c5d32b4224af5ad40dad756a1f0cd521cf77249d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
